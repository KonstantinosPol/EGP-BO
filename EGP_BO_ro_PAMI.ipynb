{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH0aLWqEA_R_"
      },
      "outputs": [],
      "source": [
        "!pip install botorch\n",
        "!pip3 install pyro-ppl\n",
        "!pip install scipy==1.4.1\n",
        "\n",
        "import numpy as np \n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "import scipy.linalg as spla\n",
        "import numpy.linalg as npla\n",
        "import sklearn.gaussian_process as gp\n",
        "import torch\n",
        "from botorch.models import SingleTaskGP\n",
        "from botorch.fit import fit_gpytorch_model\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from botorch.test_functions import Hartmann\n",
        "from botorch.utils.transforms import unnormalize\n",
        "import math\n",
        "import gpytorch\n",
        "from matplotlib import pyplot as plt\n",
        "from gpytorch.means import ConstantMean\n",
        "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
        "from gpytorch.distributions import MultivariateNormal\n",
        "from botorch.models.gpytorch import GPyTorchModel\n",
        "from gpytorch.distributions import MultivariateNormal\n",
        "from gpytorch.means import ConstantMean\n",
        "from gpytorch.models import ExactGP\n",
        "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
        "from gpytorch.likelihoods import GaussianLikelihood\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from gpytorch.priors import GammaPrior\n",
        "from botorch.acquisition import UpperConfidenceBound\n",
        "from botorch.acquisition import ProbabilityOfImprovement\n",
        "from botorch.acquisition.analytic import ExpectedImprovement\n",
        "from botorch.optim import optimize_acqf\n",
        "from botorch.acquisition.analytic import ExpectedImprovement\n",
        "# Dictionary for parameters\n",
        "ker_vars = [10**(-4), 10**(-3), 10**(-2), 10**(-1), 1, 10**(1), 10**(2), 10**(3), 10**(4), 10**(5), 10**(6)]\n",
        "ker_vars = np.array(ker_vars)\n",
        "n_learners = 11\n",
        "params = {'tau': 0.2*np.ones(n_learners) ,\n",
        "          'nu': 50*np.ones(n_learners), 'kern_shutdown_flg': 10**(-16),\n",
        "          'ker_params': ker_vars, 'M': n_learners, 'n_RF': 50,\n",
        "          'prob_bias': 0, 'sigma_bias': 0}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9NF0RBKAsCr"
      },
      "outputs": [],
      "source": [
        "!pip install Box2D\n",
        "!pip install pygame\n",
        "from push_world import *\n",
        "import sys\n",
        "import random\n",
        "\n",
        "# Copyright (c) 2017 Zi Wang\n",
        "def robot_pushing_3d(\n",
        "    rx: float,\n",
        "    ry: float,\n",
        "    duration: float,\n",
        "    ox: float,\n",
        "    oy: float,\n",
        ") -> float:\n",
        "    simu_steps = int(10 * duration)\n",
        "    # set it to False if no gui needed\n",
        "    world = b2WorldInterface(False)\n",
        "    oshape, osize, ofriction, odensity, bfriction, hand_shape, hand_size  = 'circle', 1, 0.01, 0.05, 0.01, 'rectangle', (0.3,1) \n",
        "    #thing,base = make_thing(500, 500, world, oshape, osize, ofriction, odensity, bfriction, (ox,oy))\n",
        "    thing,base = make_thing(500, 500, world, oshape, osize, ofriction, odensity, bfriction, (0,0))\n",
        "\n",
        "    init_angle = np.arctan(ry/rx)\n",
        "    robot = end_effector(world, (rx,ry), base, init_angle, hand_shape, hand_size)\n",
        "    final_location = simu_push(world, thing, robot, base, simu_steps)\n",
        "    ret = np.linalg.norm(np.array([ox, oy]) - final_location)\n",
        "    return ret\n",
        "\n",
        "def robot_pushing_4d(\n",
        "    rx: float,\n",
        "    ry: float,\n",
        "    duration: float,\n",
        "    init_angle: float,\n",
        "    ox: float,\n",
        "    oy: float,\n",
        ") -> float:\n",
        "    simu_steps = int(10 * duration)\n",
        "    # set it to False if no gui needed\n",
        "    world = b2WorldInterface(False)\n",
        "    oshape, osize, ofriction, odensity, bfriction, hand_shape, hand_size  = 'circle', 1, 0.01, 0.05, 0.01, 'rectangle', (0.3,1) \n",
        "    #thing,base = make_thing(500, 500, world, oshape, osize, ofriction, odensity, bfriction, (ox,oy))\n",
        "    thing,base = make_thing(500, 500, world, oshape, osize, ofriction, odensity, bfriction, (0,0))\n",
        "    xvel = -rx;\n",
        "    yvel = -ry;\n",
        "    regu = np.linalg.norm([xvel,yvel])\n",
        "    xvel = xvel / regu * 10;\n",
        "    yvel = yvel / regu * 10;\n",
        "    init_angle = np.arctan(ry/rx)\n",
        "    robot = end_effector(world, (rx,ry), base, init_angle, hand_shape, hand_size)\n",
        "    final_location = simu_push2(world, thing, robot, base, xvel, yvel, simu_steps)\n",
        "    ret = np.linalg.norm(np.array([ox, oy]) - final_location)\n",
        "    return ret\n",
        "    \n",
        "def robot_pushing_2x2(\n",
        "    rx: float,\n",
        "    ry: float,\n",
        "    xvel: float,\n",
        "    yvel: float,\n",
        "    duration: float,\n",
        "    init_angle: float,\n",
        "    rx2: float,\n",
        "    ry2 : float,\n",
        "    xvel2: float,\n",
        "    yvel2: float,\n",
        "    duration2: float,\n",
        "    init_angle2: float,\n",
        "    rtor: float,\n",
        "    rtor2: float,\n",
        "    gx: float,\n",
        "    gy: float,\n",
        "    gx2: float,\n",
        "    gy2: float,\n",
        "    \n",
        ") -> float:\n",
        "    simu_steps = int(10 * duration)\n",
        "    simu_steps2 = int(10 * duration2)\n",
        "    # set it to False if no gui needed    \n",
        "    world = b2WorldInterface(False)\n",
        "    oshape, osize, ofriction, odensity, bfriction, hand_shape, hand_size  = 'circle', 1, 0.01, 0.05, 0.01, 'rectangle', (1,0.3) #'circle', 0.3#\n",
        "    #thing,base = make_thing(500, 500, world, oshape, osize, ofriction, odensity, bfriction, (0,0))\n",
        "    base = make_base(500, 500, world)\n",
        "    thing = make_1thing(base, world, 'rectangle', (0.5,0.5), ofriction, odensity, (0, 2))\n",
        "    thing2 = make_1thing(base, world, 'circle', 1, ofriction, odensity, (0,-2))\n",
        "    #xvel = np.cos(init_angle)*5;\n",
        "    #yvel = np.sin(init_angle)*5;\n",
        "    robot = end_effector(world, (rx,ry), base, init_angle, hand_shape, hand_size)\n",
        "    robot2 = end_effector(world, (rx2,ry2), base, init_angle2, hand_shape, hand_size)\n",
        "    (ret1, ret2) = simu_push_2robot2thing(world, thing, thing2, robot, robot2, base, xvel, yvel, xvel2, yvel2, rtor, rtor2, simu_steps, simu_steps2)\n",
        "    #print ret1, ret2\n",
        "    ret1 = np.linalg.norm(np.array([gx, gy]) - ret1)\n",
        "    ret2 = np.linalg.norm(np.array([gx2, gy2]) - ret2)\n",
        "    return ret1+ret2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG2dHIb9SlDt"
      },
      "outputs": [],
      "source": [
        "# Create dataset \n",
        "\n",
        "def syn_fun(X, syn_function, tau):\n",
        "\n",
        "  if syn_function == \"robot_push3d\":\n",
        "    x1 = X[:,0]\n",
        "    x2 = X[:,1]\n",
        "    x3 = X[:,2]\n",
        "    des_pos = [-4.143838793976, -3.59614858230749]\n",
        "    y = []\n",
        "    for i in range(X.shape[0]):\n",
        "      y.append(-robot_pushing_3d(x1[i],x2[i],x3[i],des_pos[0],des_pos[1])) \n",
        "    y += random.normalvariate(0, tau)\n",
        "    return y\n",
        "  elif syn_function == \"robot_push4d\":\n",
        "    x1 = X[:,0]\n",
        "    x2 = X[:,1]\n",
        "    x3 = X[:,2]\n",
        "    x4 = X[:,3]\n",
        "    des_pos = [-2.406383184994124, 4.831160212812405]\n",
        "    y = []\n",
        "    for i in range(X.shape[0]):\n",
        "      y.append(-robot_pushing_4d(x1[i],x2[i],x3[i],x4[i],des_pos[0],des_pos[1])) \n",
        "    y += random.normalvariate(0, tau)\n",
        "    return y\n",
        "\n",
        "\n",
        "def obj_syn_fun(X, syn_function):\n",
        "\n",
        "  if syn_function == \"robot_push3d\":\n",
        "    x1 = X[:,0]\n",
        "    x2 = X[:,1]\n",
        "    x3 = X[:,2]\n",
        "    des_pos = [-4.143838793976, -3.59614858230749]\n",
        "    y = []\n",
        "    for i in range(X.shape[0]):\n",
        "      y.append(-robot_pushing_3d(x1[i],x2[i],x3[i],des_pos[0],des_pos[1]))  \n",
        "    return y\n",
        "  elif syn_function == \"robot_push4d\":\n",
        "    x1 = X[:,0]\n",
        "    x2 = X[:,1]\n",
        "    x3 = X[:,2]\n",
        "    x4 = X[:,3]\n",
        "    des_pos =  [-2.406383184994124, 4.831160212812405]\n",
        "    y = []\n",
        "    for i in range(X.shape[0]):\n",
        "      y.append(-robot_pushing_4d(x1[i],x2[i],x3[i],x4[i],des_pos[0],des_pos[1])) \n",
        "    return y\n",
        "\n",
        "\n",
        "syn_function = \"robot_push3d\"\n",
        "reinit_key = 0\n",
        "\n",
        "if syn_function == \"robot_push3d\":\n",
        "  bnds = ((-5,5), (-5,5), (0.99,30))\n",
        "  benchmark_value = 0\n",
        "  X = np.zeros((10,3))               #each row -> feature vector \n",
        "  X0_points = np.zeros((40,3))\n",
        "  for i in range(40):\n",
        "    if i < 10:\n",
        "      X[i,0] = random.uniform(-5, 5)\n",
        "      X[i,1] = random.uniform(-5, 5)\n",
        "      X[i,2] = random.uniform(1, 30)\n",
        "\n",
        "    X0_points[i,0] = random.uniform(-5,5)\n",
        "    X0_points[i,1] = random.uniform(-5,5)\n",
        "    X0_points[i,2] = random.uniform(1,30)\n",
        "  low_bound = [-5, -5, 1]\n",
        "  up_bound = [5, 5,30]\n",
        "\n",
        "elif syn_function == \"robot_push4d\":\n",
        "  bnds = ((-5,5), (-5,5), (0.99,30),(0.001,2*np.pi))\n",
        "  benchmark_value = 0\n",
        "  X = np.zeros((10,4))              #each row -> feature vector \n",
        "  X0_points = np.zeros((40,4))\n",
        "  for i in range(40):\n",
        "    if i < 10:\n",
        "      X[i,0] = random.uniform(-5, 5)\n",
        "      X[i,1] = random.uniform(-5, 5)\n",
        "      X[i,2] = random.uniform(1, 30)\n",
        "      X[i,3] = random.uniform(0, 2*np.pi)\n",
        "    X0_points[i,0] = random.uniform(-5,5)\n",
        "    X0_points[i,1] = random.uniform(-5,5)\n",
        "    X0_points[i,2] = random.uniform(1,30)\n",
        "    X0_points[i,3] = random.uniform(0,2*np.pi)\n",
        "  low_bound = [-5, -5, 1,0]\n",
        "  up_bound = [5, 5,30, 2*np.pi]\n",
        "\n",
        "\n",
        "print(\"===============================================================\")\n",
        "print('X=',X)\n",
        "tau = params[\"tau\"][0]\n",
        "y = syn_fun(X, syn_function, tau)\n",
        "print('y=',y)\n",
        "print(\"===============================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u_EeKhrZUYMg"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import gpytorch\n",
        "import pyro\n",
        "from pyro.infer.mcmc import NUTS, MCMC, HMC\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trnUoxFeYdmE"
      },
      "source": [
        "Full Bayesian GP for hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "88nunYFmYcSF"
      },
      "outputs": [],
      "source": [
        "from gpytorch.priors import LogNormalPrior, NormalPrior, UniformPrior\n",
        "def full_Bayesian_hyperparams(X,params,num_samples,warmup_steps,num_points):\n",
        "  train_X = torch.from_numpy(X)\n",
        "  tau = params[\"tau\"][0]\n",
        "  y = syn_fun(X, syn_function, tau)\n",
        "  train_Y = torch.from_numpy(y)\n",
        "  train_Y = torch.reshape(train_Y,(num_points,1))\n",
        "  # We will use the simplest form of GP model, exact inference\n",
        "  class ExactGPModel(gpytorch.models.ExactGP):\n",
        "      def __init__(self, train_x, train_y, likelihood):\n",
        "          super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "          self.mean_module = gpytorch.means.ConstantMean()\n",
        "          self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "      def forward(self, x):\n",
        "          mean_x = self.mean_module(x)\n",
        "          covar_x = self.covar_module(x)\n",
        "          return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "  # Use a positive constraint instead of usual GreaterThan(1e-4) so that LogNormal has support over full range.\n",
        "  likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.Positive())\n",
        "  model = ExactGPModel(train_X, train_Y, likelihood)\n",
        "  model.mean_module.register_prior(\"mean_prior\", UniformPrior(0, 0.0001), \"constant\")\n",
        "  model.covar_module.base_kernel.register_prior(\"lengthscale_prior\", UniformPrior(10**(-3), 10**3), \"lengthscale\")\n",
        "  model.covar_module.register_prior(\"outputscale_prior\", UniformPrior(1, 100), \"outputscale\")\n",
        "  likelihood.register_prior(\"noise_prior\", UniformPrior(0.15, 0.25), \"noise\")\n",
        "\n",
        "  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "\n",
        "  def pyro_model(x, y):\n",
        "      with gpytorch.settings.fast_computations(False, False, False):\n",
        "          sampled_model = model.pyro_sample_from_prior()\n",
        "          output = sampled_model.likelihood(sampled_model(x))\n",
        "          pyro.sample(\"obs\", output, obs=y)\n",
        "      return y\n",
        "\n",
        "  nuts_kernel = NUTS(pyro_model)\n",
        "  #mcmc_run = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps, disable_progbar=smoke_test)\n",
        "  mcmc_run = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)\n",
        "  mcmc_run.run(train_X, train_Y)\n",
        "  #model.pyro_load_from_samples(mcmc_run.get_samples())\n",
        "  samples = mcmc_run.get_samples()\n",
        "  ker_var_vec = samples[\"covar_module.base_kernel.lengthscale_prior\"].numpy()\n",
        "  ker_var_vec = np.reshape(ker_var_vec,(1,num_samples))\n",
        "  nu_vec = samples[\"covar_module.outputscale_prior\"].numpy()\n",
        "  nu_vec = np.reshape(nu_vec,(1,num_samples))\n",
        "  tau_vec = samples[\"likelihood.noise_prior\"].numpy()\n",
        "  tau_vec = np.reshape(tau_vec,(1,num_samples))\n",
        "  return ker_var_vec,nu_vec,tau_vec\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9Vvc8sB-dPo"
      },
      "source": [
        "Codes for RF-based single GP & EGPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSOkve4Aar1r"
      },
      "outputs": [],
      "source": [
        "def GP_hyp_param_optim(X,y,ker_var,ker_lscale_type,tau,syn_function):\n",
        "  \n",
        "  if ker_lscale_type == \"fixed\":\n",
        "    kernel = 1.0 * gp.kernels.RBF(length_scale=1.0, length_scale_bounds=(ker_var, ker_var))\n",
        "    model_gp = gp.GaussianProcessRegressor(kernel=kernel,alpha=tau,n_restarts_optimizer=10,normalize_y=False)\n",
        "    model_gp.fit(X, y)\n",
        "    learned_kernel = model_gp.kernel_\n",
        "    temp = learned_kernel.get_params(deep=True)[\"k1\"]\n",
        "    nu = temp.get_params()[\"constant_value\"]\n",
        "    ker_len = ker_var \n",
        "  else:\n",
        "    kernel = 1.0 * gp.kernels.RBF(length_scale=1.0)\n",
        "    model_gp = gp.GaussianProcessRegressor(kernel=kernel,alpha=tau,n_restarts_optimizer=10,normalize_y=False)\n",
        "    model_gp.fit(X, y)\n",
        "    learned_kernel = model_gp.kernel_\n",
        "    ker_len = learned_kernel.get_params()[\"k2__length_scale\"]\n",
        "    temp = learned_kernel.get_params(deep=True)[\"k1\"]\n",
        "    nu = temp.get_params()[\"constant_value\"]\n",
        "  return nu, ker_len\n",
        "\n",
        "def RF_gen(len,D,ker_var):\n",
        "  V = np.random.randn(D,len)\n",
        "  V = (1/np.sqrt(ker_var))*V\n",
        "  return V \n",
        "\n",
        "def RF_phi(x,V,D):\n",
        "  Vtx = V@x  \n",
        "  phi_xt=(1/np.sqrt(D))*np.vstack((np.cos(Vtx),np.sin(Vtx)))\n",
        "  phi_xt = np.reshape(phi_xt,(2*D,1))\n",
        "  return phi_xt\n",
        "\n",
        "def single_predict(phi_xt1, Sigma_t, tau, theta_t):\n",
        "  mu_p = np.dot(theta_t.T, phi_xt1)\n",
        "  var_p = np.dot(phi_xt1.T, Sigma_t@phi_xt1) + tau\n",
        "  return mu_p, var_p\n",
        "\n",
        "def single_correct(theta,Sigma,sig_hat_sq,phi_xt1,y_t1,y_hat_t1,D,sigma_bias):\n",
        "  sig_h_inv = (1/sig_hat_sq)\n",
        "  Sphi = Sigma@phi_xt1\n",
        "  G = sig_h_inv*(Sphi)\n",
        "  theta = theta+G*(y_t1-y_hat_t1)\n",
        "  Sigma_new = Sigma-sig_hat_sq*(G@G.T)\n",
        "  return theta, Sigma_new\n",
        "\n",
        "def gauss_sample(theta_t,Sigma):\n",
        "  mean = np.squeeze(theta_t)\n",
        "  cov = Sigma\n",
        "  samp = np.random.multivariate_normal(mean,cov, 1)\n",
        "  return samp.T\n",
        "\n",
        "\n",
        "def optimize_x(theta_samp,V,D,X0_points,bnds):\n",
        "  func = lambda x: -np.dot(RF_phi(x,V,D).T,theta_samp)\n",
        "  n_points = X0_points.shape[0]\n",
        "  best_x = []\n",
        "  best_fun = -np.inf \n",
        "  for i in range(n_points):\n",
        "    x0 = X0_points[i,:]\n",
        "    res = minimize(func, x0, method=\"L-BFGS-B\", bounds=bnds)\n",
        "    if -res.fun > best_fun:\n",
        "      best_x = res.x\n",
        "      best_fun = -res.fun\n",
        "  return best_x, best_fun\n",
        "\n",
        "    \n",
        "def simple_regret(obj_f_list, benchmark_value):\n",
        "  regret_list = []\n",
        "  for i in range(len(obj_f_list)):\n",
        "    regret_list.append(np.max(obj_f_list[0:i+1]))\n",
        "  regret_list = benchmark_value*np.ones(len(obj_f_list)) - regret_list\n",
        "  return regret_list \n",
        "\n",
        "def post_update_init(X,y,V,D,sigma_bias,tau,theta_t,Sigma):\n",
        "  n_points = X.shape[0]\n",
        "  len_x = X.shape[1]\n",
        "  for i in range(n_points):\n",
        "    x_t = X[i,:]\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    phi_xt = RF_phi(x_t,V,D)\n",
        "    Sigma = Sigma + sigma_bias*np.identity(2*D)\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma, tau, theta_t)\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    y_t = y[i]\n",
        "    [theta_t,Sigma]=single_correct(theta_t,Sigma,var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "  return theta_t, Sigma \n",
        "\n",
        "      \n",
        "\n",
        "def main_GP(X,y,params,X0_points,bnds,syn_function, reinit_key):\n",
        "  nu_vec = params[\"nu\"]\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = params[\"tau\"]\n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  tau = tau_vec[0]\n",
        "  ker_vars = params[\"ker_params\"]\n",
        "  ker_var = ker_vars[3]              #Just for initialization\n",
        "  [nu, ker_var] = GP_hyp_param_optim(X,y,ker_var,\"nfixed\",tau,syn_function)\n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  X_list = x_t.T\n",
        "  # Initialization of theta_t and Sigma of the posterior \n",
        "  theta_t = np.zeros((2*D,1))\n",
        "  Sigma = nu*np.identity(2*D)\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  y_list = []\n",
        "  obj_f_list = []\n",
        "  # Generate vectors v for the RF\n",
        "  V = RF_gen(len_x,D,ker_var)         \n",
        "  theta_t, Sigma = post_update_init(X,y,V,D,sigma_bias,tau,theta_t,Sigma)\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  for t in range(iter):\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)         \n",
        "    y_list.append(y_t)\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "    \n",
        "    if reinit_key == 1:\n",
        "      if t%1 == 0:\n",
        "        [nu, ker_var] = GP_hyp_param_optim(X_list,y_list,ker_var,\"nfixed\",tau,syn_function)\n",
        "        theta_t = np.zeros((2*D,1))\n",
        "        Sigma = nu*np.identity(2*D)\n",
        "        V = RF_gen(len_x,D,ker_var)\n",
        "        theta_t, Sigma = post_update_init(X_list,y_list,V,D,sigma_bias,tau,theta_t,Sigma)\n",
        "\n",
        "    # Construct RF vector for x_t \n",
        "    phi_xt = RF_phi(x_t,V,D)\n",
        "    Sigma = Sigma + sigma_bias*np.identity(2*D)\n",
        "    # Prediction step\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma, tau, theta_t)\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    y_pred.append(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    var_pred.append(var_p)\n",
        "    # Correction step\n",
        "    [theta_t,Sigma]=single_correct(theta_t,Sigma,var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "    # Sample from posterior \n",
        "    theta_samp = gauss_sample(theta_t,Sigma)\n",
        "    # Acquisition function theta_samp * RF  (Thomson sampling)\n",
        "    # Optimize acquisition function to obtain x_{t+1}\n",
        "    [x_t, obj_x_t] = optimize_x(theta_samp,V,D,X0_points,bnds)\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    X_list = np.vstack((X_list, x_t.T))\n",
        "\n",
        "  return x_t,y_pred,obj_f_list,var_pred\n",
        "\n",
        "def myGL(y,mu,var):\n",
        "  gl = (1/(np.sqrt(2*np.pi*var)))*np.exp(-((y-mu)**2)/(2*var))\n",
        "  return gl\n",
        "\n",
        "def update_weights(w_vec_in,l_vec,prob_bias):\n",
        "  w_vec_out=w_vec_in * l_vec\n",
        "  if (np.sum(w_vec_out)) == 0:\n",
        "    w_vec_out = w_vec_in\n",
        "  else:\n",
        "    w_vec_out = w_vec_out/(np.sum(w_vec_out))\n",
        "    w_vec_out = w_vec_out+prob_bias\n",
        "    w_vec_out = w_vec_out/(np.sum(w_vec_out))\n",
        "  return w_vec_out \n",
        "\n",
        "def main_EGP(X,y,params,X0_points,bnds,syn_function, reinit_key):\n",
        "  nu_vec = params[\"nu\"]\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]             #for noisy function measurements\n",
        "  ker_vars = params[\"ker_params\"]\n",
        "  M = params[\"M\"]\n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  prob_bias = params[\"prob_bias\"]\n",
        "\n",
        "  # Initialization of theta_t of posterior for all M experts\n",
        "  theta_t = np.zeros((M,2*D,1))\n",
        "  Sigma = np.zeros((M,2*D,2*D))\n",
        "  for m in range(M):\n",
        "    # Hyperparameter tuning for all M learners\n",
        "    [nu_vec[m], ker_vars[m]] = GP_hyp_param_optim(X,y,ker_vars[m],\"fixed\",tau_vec[m],syn_function)\n",
        "\n",
        "    # Initialization of Sigma of posterior for all M experts\n",
        "    Sigma[m,:,:] = nu_vec[m]*np.identity(2*D)\n",
        "  \n",
        "  w_vec=(1/M)*np.ones((1,M))         #Initialization of weights \n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  X_list = x_t.T\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  y_list = []\n",
        "  obj_f_list = []\n",
        "  l_vec = np.zeros((1,M))\n",
        "  mu_vec = np.zeros((1,M))\n",
        "  var_vec = np.zeros((1,M))\n",
        "  V = np.zeros((M,D,len_x))\n",
        "  for m in range(M):\n",
        "    # Generate vectors v for the RF for all M experts\n",
        "    V[m,:,:] = RF_gen(len_x,D,ker_vars[m])       \n",
        "    # Update theta and Sigma of posterior for all M experts using all init. data\n",
        "    theta_t[m,:,:], Sigma[m,:,:] = post_update_init(X,y,V[m,:,:],D,sigma_bias,tau_vec[m],theta_t[m,:,:],Sigma[m,:,:])\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  for t in range(iter):\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "    y_list.append(y_t)\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "    \n",
        "    if reinit_key == 1:\n",
        "      if t%50 == 0:\n",
        "        theta_t = np.zeros((M,2*D,1))\n",
        "        for m in range(M):\n",
        "          [nu_vec[m], ker_vars[m]] = GP_hyp_param_optim(X_list,y_list,ker_vars[m],\"fixed\",tau_vec[m],syn_function)\n",
        "          Sigma[m,:,:] = nu_vec[m]*np.identity(2*D)\n",
        "          V[m,:,:] = RF_gen(len_x,D,ker_vars[m])\n",
        "          theta_t[m,:,:], Sigma[m,:,:] = post_update_init(X_list,y_list,V[m,:,:],D,sigma_bias,tau_vec[m],theta_t[m,:,:],Sigma[m,:,:])\n",
        "          \n",
        "    for m in range(M):\n",
        "      # Construct RF vector for x_t \n",
        "      phi_xt = RF_phi(x_t,V[m,:,:],D)\n",
        "      Sigma[m,:,:] = Sigma[m,:,:] + sigma_bias*np.identity(2*D)\n",
        "      # Prediction step\n",
        "      [mu_p, var_p]=single_predict(phi_xt, Sigma[m,:,:], tau_vec[m], theta_t[m,:,:])\n",
        "      mu_p = np.squeeze(mu_p)\n",
        "      mu_vec[0,m] = mu_p\n",
        "      var_p = np.squeeze(var_p)\n",
        "      var_vec[0,m] = var_p\n",
        "      # Correction step\n",
        "      [theta_t[m,:,:],Sigma[m,:,:]]=single_correct(theta_t[m,:,:],Sigma[m,:,:],var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "      gl = myGL(y_t,mu_p,var_p)\n",
        "      gl = np.squeeze(gl)\n",
        "      l_vec[0,m] = myGL(y_t,mu_p,var_p)\n",
        "  \n",
        "    w_vec=update_weights(w_vec,l_vec,prob_bias)\n",
        "    # Choose specific expert based on w pmf \n",
        "    w_pmf = np.squeeze(w_vec)\n",
        "    ind_exp = np.random.choice(np.arange(1, M+1), p=w_pmf)\n",
        " \n",
        "    # Sample from posterior \n",
        "    theta_samp = gauss_sample(theta_t[ind_exp-1,:,:],Sigma[ind_exp-1,:,:])\n",
        "\n",
        "    # Acquisition function theta_samp * RF  (Thomson sampling)\n",
        "    # Optimize acquisition function to obtain x_{t+1}\n",
        " \n",
        "    [x_t, obj_x_t] = optimize_x(theta_samp,V[ind_exp-1,:,:],D,X0_points,bnds)\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    X_list = np.vstack((X_list, x_t.T))\n",
        "\n",
        "  return x_t,y_pred,obj_f_list,var_pred\n",
        "\n",
        " \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm2YBLHSwBYp"
      },
      "source": [
        "EGPs with different types of kernels combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "04MPJYV5xqzf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# include Matern kernel\n",
        "\n",
        "def GP_hyp_param_optim_matern(X,y,ker_var,ker_lscale_type,tau,syn_function,nu_mat):\n",
        "  \n",
        "  if ker_lscale_type == \"fixed\":\n",
        "    kernel = 1.0 * gp.kernels.Matern(length_scale=1.0, length_scale_bounds=(ker_var, ker_var), nu = nu_mat)\n",
        "    model_gp = gp.GaussianProcessRegressor(kernel=kernel,alpha=tau,n_restarts_optimizer=10,normalize_y=False)\n",
        "    model_gp.fit(X, y)\n",
        "    learned_kernel = model_gp.kernel_\n",
        "    temp = learned_kernel.get_params(deep=True)[\"k1\"]\n",
        "    nu2 = temp.get_params()[\"constant_value\"]\n",
        "    ker_len = ker_var \n",
        "  else:\n",
        "    kernel = 1.0 * gp.kernels.Matern(length_scale=1.0, nu = nu_mat)\n",
        "    model_gp = gp.GaussianProcessRegressor(kernel=kernel,alpha=tau,n_restarts_optimizer=10,normalize_y=False)\n",
        "    model_gp.fit(X, y)\n",
        "    learned_kernel = model_gp.kernel_\n",
        "    ker_len = learned_kernel.get_params()[\"k2__length_scale\"]\n",
        "    temp = learned_kernel.get_params(deep=True)[\"k1\"]\n",
        "    nu2 = temp.get_params()[\"constant_value\"]\n",
        "  return nu2, ker_len\n",
        "\n",
        "# include ARD kernel \n",
        "\n",
        "def GP_hyp_param_optim_ARD(X,y,len_x,tau,syn_function):\n",
        "  kernel = 1.0 * gp.kernels.RBF(length_scale= np.ones(len_x))\n",
        "  model_gp = gp.GaussianProcessRegressor(kernel=kernel,alpha=tau,n_restarts_optimizer=10,normalize_y=False)\n",
        "  model_gp.fit(X, y)\n",
        "  learned_kernel = model_gp.kernel_\n",
        "  ker_len = learned_kernel.get_params()[\"k2__length_scale\"]\n",
        "  temp = learned_kernel.get_params(deep=True)[\"k1\"]\n",
        "  nu = temp.get_params()[\"constant_value\"]\n",
        "  return nu, ker_len\n",
        "\n",
        "def RF_gen_ARD(len,D,ker_var_vec):\n",
        "  V = np.random.randn(D,len)\n",
        "  V = V/np.sqrt(ker_var_vec)\n",
        "  return V \n",
        "\n",
        "def RF_gen_Matern(len,D,ker_var,nu_mat):\n",
        "  V = np.random.randn(D,len)\n",
        "  V = (1/np.sqrt(ker_var))*V\n",
        "  V = V/np.sqrt(np.random.gamma(shape=nu_mat, scale=1.0 / nu_mat, size=(D, 1)))\n",
        "  return V \n",
        "\n",
        "\n",
        "def main_GP_matern(X,y,params,X0_points,bnds,syn_function, reinit_key):\n",
        "  nu_vec = params[\"nu\"]\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]\n",
        "  ker_vars = params[\"ker_params\"]\n",
        "  ker_var = ker_vars[3]\n",
        "  [nu, ker_var] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau,syn_function,nu_mat=2.5)\n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  X_list = x_t.T\n",
        "  # Initialization of theta_t and Sigma of the posterior \n",
        "  theta_t = np.zeros((2*D,1))\n",
        "  Sigma = nu*np.identity(2*D)\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  y_list = []\n",
        "  obj_f_list = []\n",
        "  # Generate vectors v for the RF\n",
        "  V = RF_gen_Matern(len_x,D,ker_var,nu_mat = 2.5)        \n",
        "  theta_t, Sigma = post_update_init(X,y,V,D,sigma_bias,tau,theta_t,Sigma)\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  for t in range(iter):\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "    y_list.append(y_t)\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "    \n",
        "    if reinit_key == 1:\n",
        "      if t%1 == 0:\n",
        "        [nu, ker_var] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau,syn_function,nu_mat=2.5)\n",
        "        theta_t = np.zeros((2*D,1))\n",
        "        Sigma = nu*np.identity(2*D)\n",
        "        V = RF_gen_Matern(len_x,D,ker_var,nu_mat = 2.5)\n",
        "        theta_t, Sigma = post_update_init(X_list,y_list,V,D,sigma_bias,tau,theta_t,Sigma)\n",
        "\n",
        "    # Construct RF vector for x_t \n",
        "    phi_xt = RF_phi(x_t,V,D)\n",
        "    Sigma = Sigma + sigma_bias*np.identity(2*D)\n",
        "    # Prediction step\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma, tau, theta_t)\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    y_pred.append(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    var_pred.append(var_p)\n",
        "    # Correction step\n",
        "    [theta_t,Sigma]=single_correct(theta_t,Sigma,var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "    # Sample from posterior \n",
        "    theta_samp = gauss_sample(theta_t,Sigma)\n",
        "\n",
        "    # Acquisition function theta_samp * RF  (Thomson sampling)\n",
        "    # Optimize acquisition function to obtain x_{t+1}\n",
        "\n",
        "    [x_t, obj_x_t] = optimize_x(theta_samp,V,D,X0_points,bnds)\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    X_list = np.vstack((X_list, x_t.T))\n",
        "\n",
        "  return x_t,y_pred,obj_f_list,var_pred\n",
        "\n",
        "def main_GP_matern2(X,y,params,X0_points,bnds,syn_function, reinit_key):\n",
        "\n",
        "  nu_vec = params[\"nu\"]\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]\n",
        "  ker_vars = params[\"ker_params\"]\n",
        "  ker_var = ker_vars[3]\n",
        "  [nu, ker_var] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau,syn_function,nu_mat=1.5)\n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  X_list = x_t.T\n",
        "  # Initialization of theta_t and Sigma of the posterior \n",
        "  theta_t = np.zeros((2*D,1))\n",
        "  Sigma = nu*np.identity(2*D)\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  y_list = []\n",
        "  obj_f_list = []\n",
        "  # Generate vectors v for the RF\n",
        "  V = RF_gen_Matern(len_x,D,ker_var,nu_mat = 1.5)        \n",
        "  theta_t, Sigma = post_update_init(X,y,V,D,sigma_bias,tau,theta_t,Sigma)\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  for t in range(iter):\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "    y_list.append(y_t)\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "    \n",
        "    if reinit_key == 1:\n",
        "      if t%1 == 0:\n",
        "        [nu, ker_var] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau,syn_function,nu_mat=1.5)\n",
        "        theta_t = np.zeros((2*D,1))\n",
        "        Sigma = nu*np.identity(2*D)\n",
        "        V = RF_gen_Matern(len_x,D,ker_var,nu_mat = 2.5)\n",
        "        theta_t, Sigma = post_update_init(X_list,y_list,V,D,sigma_bias,tau,theta_t,Sigma)\n",
        "\n",
        "    # Construct RF vector for x_t \n",
        "    phi_xt = RF_phi(x_t,V,D)\n",
        "    Sigma = Sigma + sigma_bias*np.identity(2*D)\n",
        "    # Prediction step\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma, tau, theta_t)\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    #print(\"y_pred=\", mu_p)\n",
        "    y_pred.append(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    var_pred.append(var_p)\n",
        "    # Correction step\n",
        "    [theta_t,Sigma]=single_correct(theta_t,Sigma,var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "    # Sample from posterior \n",
        "    theta_samp = gauss_sample(theta_t,Sigma)\n",
        "    # Acquisition function theta_samp * RF  (Thomson sampling)\n",
        "    # Optimize acquisition function to obtain x_{t+1}\n",
        "\n",
        "    [x_t, obj_x_t] = optimize_x(theta_samp,V,D,X0_points,bnds)\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    X_list = np.vstack((X_list, x_t.T))\n",
        "\n",
        "  return x_t,y_pred,obj_f_list,var_pred\n",
        "\n",
        "def main_GP_ARD(X,y,params,X0_points,bnds,syn_function, reinit_key):\n",
        "\n",
        "  nu_vec = params[\"nu\"]\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]\n",
        "  [nu, ker_vars] = GP_hyp_param_optim_ARD(X,y,X.shape[1],tau,syn_function)\n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  X_list = x_t.T\n",
        "  # Initialization of theta_t and Sigma of the posterior \n",
        "  theta_t = np.zeros((2*D,1))\n",
        "  Sigma = nu*np.identity(2*D)\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  y_list = []\n",
        "  obj_f_list = []\n",
        "  # Generate vectors v for the RF\n",
        "  V = RF_gen_ARD(len_x,D,ker_vars)\n",
        "  theta_t, Sigma = post_update_init(X,y,V,D,sigma_bias,tau,theta_t,Sigma)\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  for t in range(iter):\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "    y_list.append(y_t)\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "    \n",
        "    if reinit_key == 1:\n",
        "      if t%1 == 0:\n",
        "        [nu, ker_vars] = GP_hyp_param_optim_ARD(X,y,X.shape[1],tau,syn_function)\n",
        "        theta_t = np.zeros((2*D,1))\n",
        "        Sigma = nu*np.identity(2*D)\n",
        "        V = RF_gen_ARD(len_x,D,ker_vars)\n",
        "        theta_t, Sigma = post_update_init(X_list,y_list,V,D,sigma_bias,tau,theta_t,Sigma)\n",
        "\n",
        "\n",
        "    # Construct RF vector for x_t \n",
        "    phi_xt = RF_phi(x_t,V,D)\n",
        "    Sigma = Sigma + sigma_bias*np.identity(2*D)\n",
        "    # Prediction step\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma, tau, theta_t)\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    y_pred.append(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    var_pred.append(var_p)\n",
        "    # Correction step\n",
        "    [theta_t,Sigma]=single_correct(theta_t,Sigma,var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "    # Sample from posterior \n",
        "    theta_samp = gauss_sample(theta_t,Sigma)\n",
        " \n",
        "    # Acquisition function theta_samp * RF  (Thomson sampling)\n",
        "    # Optimize acquisition function to obtain x_{t+1}\n",
        "\n",
        "    [x_t, obj_x_t] = optimize_x(theta_samp,V,D,X0_points,bnds)\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    X_list = np.vstack((X_list, x_t.T))\n",
        "\n",
        "  return x_t,y_pred,obj_f_list,var_pred\n",
        "\n",
        "def main_EGP_rbfmaternard(X,y,params,X0_points,bnds,syn_function, reinit_key):\n",
        "  \n",
        "  M = 4 \n",
        "  nu_vec = np.ones(M)\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = 0.2*np.ones(M)\n",
        "  ker_var = 0.1 #initialization\n",
        "  ker_vars = [] \n",
        "  theta_t = np.zeros((M,2*D,1))\n",
        "  Sigma = np.zeros((M,2*D,2*D))\n",
        "  # Ensemble of 4 distinct kernels: 1) ARD 2) RBF 3) Matern v=1.5 4) Matern v=2.5\n",
        "  for m in range(M):\n",
        "    # Hyperparameter tuning for all M learners\n",
        "    if m==0: \n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim_ARD(X,y,X.shape[1],tau_vec[m],syn_function)\n",
        "      temp = np.squeeze(temp)\n",
        "      ker_vars.append(temp)\n",
        "    elif m==1:\n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function)\n",
        "      ker_vars.append(np.squeeze(temp))\n",
        "    elif m==2:\n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function,nu_mat=1.5)\n",
        "      ker_vars.append(np.squeeze(temp))\n",
        "    else:\n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function,nu_mat=2.5)\n",
        "      ker_vars.append(np.squeeze(temp))\n",
        "\n",
        "    # Initialization of Sigma of posterior for all M experts\n",
        "    Sigma[m,:,:] = nu_vec[m]*np.identity(2*D)\n",
        "  \n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  prob_bias = params[\"prob_bias\"]\n",
        "  w_vec=(1/M)*np.ones((1,M))         #Initialization of weights \n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  X_list = x_t.T\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  y_list = []\n",
        "  obj_f_list = []\n",
        "  l_vec = np.zeros((1,M))\n",
        "  mu_vec = np.zeros((1,M))\n",
        "  var_vec = np.zeros((1,M))\n",
        "  V = np.zeros((M,D,len_x))\n",
        "  for m in range(M):\n",
        "    # Generate vectors v for the RF for all M experts\n",
        "    if m==0:\n",
        "      V[m,:,:] = RF_gen_ARD(len_x,D,ker_vars[m])\n",
        "    elif m==1:\n",
        "      V[m,:,:] = RF_gen(len_x,D,ker_vars[m])\n",
        "    elif m==2:\n",
        "      V[m,:,:] = RF_gen_Matern(len_x,D,ker_vars[m],nu_mat = 1.5)\n",
        "    else:\n",
        "      V[m,:,:] = RF_gen_Matern(len_x,D,ker_vars[m],nu_mat = 2.5)\n",
        "\n",
        "    # Update theta and Sigma of posterior for all M experts using all init. data\n",
        "    theta_t[m,:,:], Sigma[m,:,:] = post_update_init(X,y,V[m,:,:],D,sigma_bias,tau_vec[m],theta_t[m,:,:],Sigma[m,:,:])\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  for t in range(iter):\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "    y_list.append(y_t)\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "\n",
        "    if reinit_key == 1:\n",
        "      if t%50 == 0:\n",
        "        theta_t = np.zeros((M,2*D,1))\n",
        "        Sigma = np.zeros((M,2*D,2*D))\n",
        "        # Ensemble of 4 distinct kernels: 1) ARD 2) RBF 3) Matern v=1.5 4) Matern v=2.5\n",
        "        for m in range(M):\n",
        "          # Hyperparameter tuning for all M learners\n",
        "          if m==0: \n",
        "            [nu_vec[m], temp] = GP_hyp_param_optim_ARD(X,y,X.shape[1],tau_vec[m],syn_function)\n",
        "            temp = np.squeeze(temp)\n",
        "            ker_vars.append(temp)\n",
        "          elif m==1:\n",
        "            [nu_vec[m], temp] = GP_hyp_param_optim(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function)\n",
        "            ker_vars.append(np.squeeze(temp))\n",
        "          elif m==2:\n",
        "            [nu_vec[m], temp] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function,nu_mat=1.5)\n",
        "            ker_vars.append(np.squeeze(temp))\n",
        "          else:\n",
        "            [nu_vec[m], temp] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function,nu_mat=2.5)\n",
        "            ker_vars.append(np.squeeze(temp))\n",
        "\n",
        "          # Initialization of Sigma of posterior for all M experts\n",
        "          Sigma[m,:,:] = nu_vec[m]*np.identity(2*D)\n",
        "\n",
        "    for m in range(M):\n",
        "      # Construct RF vector for x_t \n",
        "      phi_xt = RF_phi(x_t,V[m,:,:],D)\n",
        "      Sigma[m,:,:] = Sigma[m,:,:] + sigma_bias*np.identity(2*D)\n",
        "      # Prediction step\n",
        "      [mu_p, var_p]=single_predict(phi_xt, Sigma[m,:,:], tau_vec[m], theta_t[m,:,:])\n",
        "      mu_p = np.squeeze(mu_p)\n",
        "      mu_vec[0,m] = mu_p\n",
        "      var_p = np.squeeze(var_p)\n",
        "      var_vec[0,m] = var_p\n",
        "      # Correction step\n",
        "      [theta_t[m,:,:],Sigma[m,:,:]]=single_correct(theta_t[m,:,:],Sigma[m,:,:],var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "      gl = myGL(y_t,mu_p,var_p)\n",
        "      gl = np.squeeze(gl)\n",
        "      l_vec[0,m] = myGL(y_t,mu_p,var_p)\n",
        "    \n",
        "    w_vec=update_weights(w_vec,l_vec,prob_bias)\n",
        "    # Choose specific expert based on w pmf \n",
        "    w_pmf = np.squeeze(w_vec)\n",
        "    ind_exp = np.random.choice(np.arange(1, M+1), p=w_pmf)\n",
        "\n",
        "    # Sample from posterior \n",
        "    theta_samp = gauss_sample(theta_t[ind_exp-1,:,:],Sigma[ind_exp-1,:,:])\n",
        " \n",
        "    # Acquisition function theta_samp * RF  (Thomson sampling)\n",
        "    # Optimize acquisition function to obtain x_{t+1}\n",
        "\n",
        "    [x_t, obj_x_t] = optimize_x(theta_samp,V[ind_exp-1,:,:],D,X0_points,bnds)\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    X_list = np.vstack((X_list, x_t.T))\n",
        "    \n",
        "  return x_t,y_pred,obj_f_list,var_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcrPQLLg-SJw"
      },
      "source": [
        "Code for RF-based Bandit BO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "amUNfNIv-Qoa"
      },
      "outputs": [],
      "source": [
        "def split_data(X,y,M): \n",
        "  [T,len_x] = X.shape\n",
        "  T_m = int(T/M)\n",
        "  X_init = np.reshape(X,(M,T_m,len_x))\n",
        "  y_init = np.reshape(y,(M,T_m,1))\n",
        "  return X_init,y_init\n",
        "\n",
        "def main_BanditBO(X,y,params,X0_points,bnds,syn_function,reinit_key):\n",
        "\n",
        "  nu_vec = params[\"nu\"]\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]\n",
        "  ker_vars = params[\"ker_params\"]\n",
        "  M = params[\"M\"]\n",
        "\n",
        "  # Initialization of theta_t of posterior for all M experts\n",
        "  theta_t = np.zeros((M,2*D,1))\n",
        "  Sigma = np.zeros((M,2*D,2*D))\n",
        "\n",
        "  for m in range(M):\n",
        "    # Hyperparameter tuning for all M learners\n",
        "    [nu_vec[m], ker_vars[m]] = GP_hyp_param_optim(X,y,ker_vars[m],\"fixed\",tau_vec[m],syn_function)\n",
        "\n",
        "    # Initialization of Sigma of posterior for all M experts\n",
        "    Sigma[m,:,:] = nu_vec[m]*np.identity(2*D)\n",
        "  \n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  prob_bias = params[\"prob_bias\"]\n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  obj_f_list = []\n",
        "\n",
        "  V = np.zeros((M,D,len_x))\n",
        "  for m in range(M):\n",
        "    # Generate vectors v for the RF for all M experts\n",
        "    V[m,:,:] = RF_gen(len_x,D,ker_vars[m])       \n",
        "    # Update theta and Sigma of posterior for all M experts using all init. data\n",
        "    theta_t[m,:,:], Sigma[m,:,:] = post_update_init(X,y,V[m,:,:],D,sigma_bias,tau_vec[m],theta_t[m,:,:],Sigma[m,:,:])\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  for t in range(iter):  \n",
        "    fcstar_list = []\n",
        "    x_cand_list = np.zeros((M,len_x))\n",
        "    for m in range(M):\n",
        "      # Draw sample from posterior for bandit arm m\n",
        "      theta_samp = gauss_sample(theta_t[m,:,:],Sigma[m,:,:])\n",
        "      # Obtain x_tilde_m for bandit arm m\n",
        "      [x_t_m, ts_val] = optimize_x(theta_samp,V[m,:,:],D,X0_points,bnds)\n",
        "      ts_val = np.squeeze(ts_val)\n",
        "      fcstar_list.append(ts_val)\n",
        "      x_cand_list[m,:] = x_t_m\n",
        "    \n",
        "    # Choose arm that maximizes fcstar_list\n",
        "    m_star = np.argmax(fcstar_list)\n",
        "\n",
        "    x_t = x_cand_list[m_star,:]\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "\n",
        "    # Update posterior params. of the chosen bandit arm\n",
        "    # Construct RF vector for x_t \n",
        "    phi_xt = RF_phi(x_t,V[m_star,:,:],D)\n",
        "    Sigma[m_star,:,:] = Sigma[m_star,:,:] + sigma_bias*np.identity(2*D)\n",
        "    # Prediction step\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma[m_star,:,:], tau_vec[m_star], theta_t[m_star,:,:])\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    # Correction step\n",
        "    [theta_t[m_star,:,:],Sigma[m_star,:,:]]=single_correct(theta_t[m_star,:,:],Sigma[m_star,:,:],var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "\n",
        "  return x_t,y_pred,obj_f_list,var_pred\n",
        "\n",
        "\n",
        "def main_BanditBO_ardrbfmatern(X,y,params,X0_points,bnds,syn_function,reinit_key):\n",
        "\n",
        "  M = 4 \n",
        "  nu_vec = np.ones(M)\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = 0.2*np.ones(M)\n",
        "  ker_var = 0.1 #initialization\n",
        "  ker_vars = [] \n",
        "  theta_t = np.zeros((M,2*D,1))\n",
        "  Sigma = np.zeros((M,2*D,2*D))\n",
        "  # Ensemble of 4 distinct kernels: 1) ARD 2) RBF 3) Matern v=1.5 4) Matern v=2.5\n",
        "  for m in range(M):\n",
        "    # Hyperparameter tuning for all M learners\n",
        "    if m==0: \n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim_ARD(X,y,X.shape[1],tau_vec[m],syn_function)\n",
        "      temp = np.squeeze(temp)\n",
        "      ker_vars.append(temp)\n",
        "    elif m==1:\n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function)\n",
        "      ker_vars.append(np.squeeze(temp))\n",
        "    elif m==2:\n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function,nu_mat=1.5)\n",
        "      ker_vars.append(np.squeeze(temp))\n",
        "    else:\n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function,nu_mat=2.5)\n",
        "      ker_vars.append(np.squeeze(temp))\n",
        "\n",
        "    # Initialization of Sigma of posterior for all M experts\n",
        "    Sigma[m,:,:] = nu_vec[m]*np.identity(2*D)\n",
        "  \n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  prob_bias = params[\"prob_bias\"]\n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  obj_f_list = []\n",
        "  V = np.zeros((M,D,len_x))\n",
        "  for m in range(M):\n",
        "    # Generate vectors v for the RF for all M experts\n",
        "    if m==0:\n",
        "      V[m,:,:] = RF_gen_ARD(len_x,D,ker_vars[m])\n",
        "    elif m==1:\n",
        "      V[m,:,:] = RF_gen(len_x,D,ker_vars[m])\n",
        "    elif m==2:\n",
        "      V[m,:,:] = RF_gen_Matern(len_x,D,ker_vars[m],nu_mat = 1.5)\n",
        "    else:\n",
        "      V[m,:,:] = RF_gen_Matern(len_x,D,ker_vars[m],nu_mat = 2.5)\n",
        "    # Update theta and Sigma of posterior for all M experts using all init. data\n",
        "    theta_t[m,:,:], Sigma[m,:,:] = post_update_init(X,y,V[m,:,:],D,sigma_bias,tau_vec[m],theta_t[m,:,:],Sigma[m,:,:])\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  for t in range(iter):\n",
        "    fcstar_list = []\n",
        "    x_cand_list = np.zeros((M,len_x))\n",
        "    for m in range(M):\n",
        "      # Draw sample from posterior for bandit arm m\n",
        "      theta_samp = gauss_sample(theta_t[m,:,:],Sigma[m,:,:])\n",
        "      # Obtain x_tilde_m for bandit arm m\n",
        "      [x_t_m, ts_val] = optimize_x(theta_samp,V[m,:,:],D,X0_points,bnds)\n",
        "      ts_val = np.squeeze(ts_val)\n",
        "      fcstar_list.append(ts_val)\n",
        "      x_cand_list[m,:] = x_t_m\n",
        "    \n",
        "    # Choose arm that maximizes fcstar_list\n",
        "    m_star = np.argmax(fcstar_list)\n",
        "\n",
        "    x_t = x_cand_list[m_star,:]\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "\n",
        "    # Update posterior params. of the chosen bandit arm\n",
        "    # Construct RF vector for x_t \n",
        "    phi_xt = RF_phi(x_t,V[m_star,:,:],D)\n",
        "    Sigma[m_star,:,:] = Sigma[m_star,:,:] + sigma_bias*np.identity(2*D)\n",
        "    # Prediction step\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma[m_star,:,:], tau_vec[m_star], theta_t[m_star,:,:])\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    # Correction step\n",
        "    [theta_t[m_star,:,:],Sigma[m_star,:,:]]=single_correct(theta_t[m_star,:,:],Sigma[m_star,:,:],var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "\n",
        "  return x_t,y_pred,obj_f_list,var_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFGmy8Y3ICCZ"
      },
      "source": [
        "Code for EXP3BO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LJ56W2KZIMpY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def get_reward(theta_t_m, X_list_m, w, V, D, T, len_x): \n",
        "  max = np.NINF\n",
        "  for t in range(T):\n",
        "    if math.isnan(X_list_m[t,0]) != True:\n",
        "      x = np.array(X_list_m[t,:])\n",
        "      x = np.reshape(x,(len_x,1))\n",
        "      phi_x_t_m = RF_phi(x,V,D)\n",
        "      cand = np.dot(phi_x_t_m.T,theta_t_m)\n",
        "      if cand > max:\n",
        "        max = cand\n",
        "  reward = max/w\n",
        "  return reward\n",
        "  \n",
        "def update_omega(omega,reward,eta,M):\n",
        "  omega = omega*np.exp(eta*reward/M)\n",
        "  return omega\n",
        "\n",
        "\n",
        "def main_EXP3BO(X,y,params,X0_points,bnds,syn_function,reinit_key, benchmark_value):\n",
        "\n",
        "  nu_vec = params[\"nu\"]\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]\n",
        "  ker_vars = params[\"ker_params\"]\n",
        "  M = params[\"M\"]\n",
        "\n",
        "  # Initialization of theta_t of posterior for all M experts\n",
        "  theta_t = np.zeros((M,2*D,1))\n",
        "  Sigma = np.zeros((M,2*D,2*D))\n",
        "  for m in range(M):\n",
        "\n",
        "    # Hyperparameter tuning for all M learners\n",
        "    [nu_vec[m], ker_vars[m]] = GP_hyp_param_optim(X,y,ker_vars[m],\"fixed\",tau_vec[m],syn_function)\n",
        "\n",
        "    # Initialization of Sigma of posterior for all M experts\n",
        "    Sigma[m,:,:] = nu_vec[m]*np.identity(2*D)\n",
        "  \n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  prob_bias = params[\"prob_bias\"]\n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "  omega_vec = np.ones((1,M))              #initialize omegas\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  obj_f_list = []\n",
        "  V = np.zeros((M,D,len_x))\n",
        "  for m in range(M):\n",
        "    # Generate vectors v for the RF for all M experts\n",
        "    V[m,:,:] = RF_gen(len_x,D,ker_vars[m])       \n",
        "    # Update theta and Sigma of posterior for all M experts using all init. data\n",
        "    theta_t[m,:,:], Sigma[m,:,:] = post_update_init(X,y,V[m,:,:],D,sigma_bias,tau_vec[m],theta_t[m,:,:],Sigma[m,:,:])\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  eta = np.sqrt(M*np.log(M)/((np.e-1)*iter))\n",
        "  X_list = np.empty((M,iter+1,len_x))\n",
        "  X_list[:] = np.NaN\n",
        "  for m in range(M):\n",
        "    X_list[m,0,:] = x_t.T\n",
        "\n",
        "  for t in range(iter):\n",
        "\n",
        "    # Compute weights/ probabilities of experts \n",
        "    w_vec = (1-eta)*omega_vec/np.sum(omega_vec) + eta/M\n",
        "    w_pmf = np.squeeze(w_vec)\n",
        "  \n",
        "    # Draw expert from weight pmf\n",
        "    ind_exp = np.random.choice(np.arange(1, M+1), p=w_pmf)\n",
        "\n",
        "    # Sample from posterior \n",
        "    theta_samp = gauss_sample(theta_t[ind_exp-1,:,:],Sigma[ind_exp-1,:,:])\n",
        "    # obtain x \n",
        "    [x_t, obj_x_t] = optimize_x(theta_samp,V[ind_exp-1,:,:],D,X0_points,bnds)\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "    # include x_t to X_list for chosen expert\n",
        "    X_list[ind_exp-1,t+1,:] = x_t.T\n",
        "\n",
        "    # Update posterior params. of the chosen expert\n",
        "    # Construct RF vector for x_t \n",
        "    phi_xt = RF_phi(x_t,V[ind_exp-1,:,:],D)\n",
        "    Sigma[ind_exp-1,:,:] = Sigma[ind_exp-1,:,:] + sigma_bias*np.identity(2*D)\n",
        "    # Prediction step\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma[ind_exp-1,:,:], tau_vec[ind_exp-1], theta_t[ind_exp-1,:,:])\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    # Correction step\n",
        "    [theta_t[ind_exp-1,:,:],Sigma[ind_exp-1,:,:]]=single_correct(theta_t[ind_exp-1,:,:],Sigma[ind_exp-1,:,:],var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "    #get reward\n",
        "    reward = get_reward(theta_t[ind_exp-1,:,:], X_list[ind_exp-1,:,:], w_vec[0,ind_exp-1], V[ind_exp-1,:,:], D, iter+1, len_x)\n",
        "    if benchmark_value!= 0:\n",
        "      reward = reward/benchmark_value #normalize\n",
        "    # update omegas\n",
        "    omega_vec[0,ind_exp-1] = update_omega(omega_vec[0,ind_exp-1],reward,eta,M)\n",
        "    \n",
        "  return x_t,y_pred,obj_f_list,var_pred\n",
        "\n",
        "\n",
        "def main_EXP3BO_ardrbfmatern(X,y,params,X0_points,bnds,syn_function,reinit_key, benchmark_value):\n",
        "\n",
        "  M = 4 \n",
        "  nu_vec = np.ones(M)\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_vec = 0.2*np.ones(M)\n",
        "  ker_var = 0.1 #initialization\n",
        "  ker_vars = [] \n",
        "  theta_t = np.zeros((M,2*D,1))\n",
        "  Sigma = np.zeros((M,2*D,2*D))\n",
        "  # Ensemble of 4 distinct kernels: 1) ARD 2) RBF 3) Matern v=1.5 4) Matern v=2.5\n",
        "  for m in range(M):\n",
        "    # Hyperparameter tuning for all M learners\n",
        "    if m==0: \n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim_ARD(X,y,X.shape[1],tau_vec[m],syn_function)\n",
        "      temp = np.squeeze(temp)\n",
        "      ker_vars.append(temp)\n",
        "    elif m==1:\n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function)\n",
        "      ker_vars.append(np.squeeze(temp))\n",
        "    elif m==2:\n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function,nu_mat=1.5)\n",
        "      ker_vars.append(np.squeeze(temp))\n",
        "    else:\n",
        "      [nu_vec[m], temp] = GP_hyp_param_optim_matern(X,y,ker_var,\"nfixed\",tau_vec[m],syn_function,nu_mat=2.5)\n",
        "      ker_vars.append(np.squeeze(temp))\n",
        "\n",
        "    # Initialization of Sigma of posterior for all M experts\n",
        "    Sigma[m,:,:] = nu_vec[m]*np.identity(2*D)\n",
        "  \n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  prob_bias = params[\"prob_bias\"]\n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "  omega_vec = np.ones((1,M))              #initialize omegas\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  obj_f_list = []\n",
        "  V = np.zeros((M,D,len_x))\n",
        "  for m in range(M):\n",
        "    # Generate vectors v for the RF for all M experts\n",
        "    if m==0:\n",
        "      V[m,:,:] = RF_gen_ARD(len_x,D,ker_vars[m])\n",
        "    elif m==1:\n",
        "      V[m,:,:] = RF_gen(len_x,D,ker_vars[m])\n",
        "    elif m==2:\n",
        "      V[m,:,:] = RF_gen_Matern(len_x,D,ker_vars[m],nu_mat = 1.5)\n",
        "    else:\n",
        "      V[m,:,:] = RF_gen_Matern(len_x,D,ker_vars[m],nu_mat = 2.5)      \n",
        "    # Update theta and Sigma of posterior for all M experts using all init. data\n",
        "    theta_t[m,:,:], Sigma[m,:,:] = post_update_init(X,y,V[m,:,:],D,sigma_bias,tau_vec[m],theta_t[m,:,:],Sigma[m,:,:])\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  eta = np.sqrt(M*np.log(M)/((np.e-1)*iter))\n",
        "  X_list = np.empty((M,iter+1,len_x))\n",
        "  X_list[:] = np.NaN\n",
        "  for m in range(M):\n",
        "    X_list[m,0,:] = x_t.T\n",
        "\n",
        "  for t in range(iter):\n",
        "\n",
        "    # Compute weights/ probabilities of experts \n",
        "    w_vec = (1-eta)*omega_vec/np.sum(omega_vec) + eta/M\n",
        "    w_pmf = np.squeeze(w_vec)\n",
        "  \n",
        "    # Draw expert from weight pmf\n",
        "    ind_exp = np.random.choice(np.arange(1, M+1), p=w_pmf)\n",
        "\n",
        "    # Sample from posterior \n",
        "    theta_samp = gauss_sample(theta_t[ind_exp-1,:,:],Sigma[ind_exp-1,:,:])\n",
        "    # obtain x \n",
        "    [x_t, obj_x_t] = optimize_x(theta_samp,V[ind_exp-1,:,:],D,X0_points,bnds)\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "    # include x_t to X_list for chosen expert\n",
        "    X_list[ind_exp-1,t+1,:] = x_t.T\n",
        "\n",
        "    # Update posterior params. of the chosen expert\n",
        "    # Construct RF vector for x_t \n",
        "    phi_xt = RF_phi(x_t,V[ind_exp-1,:,:],D)\n",
        "    Sigma[ind_exp-1,:,:] = Sigma[ind_exp-1,:,:] + sigma_bias*np.identity(2*D)\n",
        "    # Prediction step\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma[ind_exp-1,:,:], tau_vec[ind_exp-1], theta_t[ind_exp-1,:,:])\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    # Correction step\n",
        "    [theta_t[ind_exp-1,:,:],Sigma[ind_exp-1,:,:]]=single_correct(theta_t[ind_exp-1,:,:],Sigma[ind_exp-1,:,:],var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "    #get reward\n",
        "    reward = get_reward(theta_t[ind_exp-1,:,:], X_list[ind_exp-1,:,:], w_vec[0,ind_exp-1], V[ind_exp-1,:,:], D, iter+1, len_x)\n",
        "    if benchmark_value!= 0:\n",
        "      reward = reward/benchmark_value #normalize\n",
        "\n",
        "    # update omegas\n",
        "    omega_vec[0,ind_exp-1] = update_omega(omega_vec[0,ind_exp-1],reward,eta,M)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "  return x_t,y_pred,obj_f_list,var_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN67hIQ64u1z"
      },
      "source": [
        "Code for Full Bayesian GP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7D78otML48a9"
      },
      "outputs": [],
      "source": [
        "def post_update_2(X,V,D,sigma_bias,tau,theta_t,Sigma):\n",
        "  n_points = X.shape[0]\n",
        "  len_x = X.shape[1]\n",
        "  y = syn_fun(X, syn_function, tau)\n",
        "  for i in range(n_points):\n",
        "    x_t = X[i,:]\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    phi_xt = RF_phi(x_t,V,D)\n",
        "    Sigma = Sigma + sigma_bias*np.identity(2*D)\n",
        "    [mu_p, var_p]=single_predict(phi_xt, Sigma, tau, theta_t)\n",
        "    mu_p = np.squeeze(mu_p)\n",
        "    var_p = np.squeeze(var_p)\n",
        "    y_t = y[i]\n",
        "    [theta_t,Sigma]=single_correct(theta_t,Sigma,var_p,phi_xt,y_t,mu_p,D,sigma_bias)\n",
        "  return theta_t, Sigma \n",
        "\n",
        "def main_full_Bayesian_GP(X,y,params,X0_points,bnds,syn_function):\n",
        "  nu_init = params[\"nu\"]\n",
        "  D = params[\"n_RF\"]\n",
        "  tau_init = params[\"tau\"]\n",
        "  M = params[\"M\"]\n",
        "  sigma_bias = params[\"sigma_bias\"]\n",
        "  prob_bias = params[\"prob_bias\"]\n",
        "  kern_shutdown_flg=params[\"kern_shutdown_flg\"]\n",
        "  w_vec=(1/M)*np.ones((1,M))         #Initialization of weights (will remain unaltered)\n",
        "  x_t = X[0,:]\n",
        "  x_t = np.array(x_t)\n",
        "  len_x = x_t.shape[0]\n",
        "  x_t = np.reshape(x_t,(len_x,1)) \n",
        "  X_list = X\n",
        "  y_pred = []\n",
        "  var_pred = []\n",
        "  y_list = []\n",
        "  obj_f_list = []\n",
        "  l_vec = np.zeros((1,M))\n",
        "  mu_vec = np.zeros((1,M))\n",
        "  var_vec = np.zeros((1,M))\n",
        "  V = np.zeros((M,D,len_x))\n",
        "  # Find hyperparams in a Bayesian way using all initial data\n",
        "  ker_vars,nu_vec,tau_vec = full_Bayesian_hyperparams(X,params,num_samples=11,warmup_steps=80,num_points= X.shape[0])\n",
        "  # Initialization of theta_t of posterior for all M experts\n",
        "  theta_t = np.zeros((M,2*D,1))\n",
        "  Sigma = np.zeros((M,2*D,2*D))\n",
        "  for m in range(M):\n",
        "    Sigma[m,:,:] = nu_vec[0,m]*np.identity(2*D)\n",
        "  for m in range(M):\n",
        "    # Generate vectors v for the RF for all M experts\n",
        "    V[m,:,:] = RF_gen(len_x,D,ker_vars[0,m])       \n",
        "    # Update theta and Sigma of posterior for all M experts using all init. data\n",
        "    theta_t[m,:,:], Sigma[m,:,:] = post_update_2(X,V[m,:,:],D,sigma_bias,tau_init[0],theta_t[m,:,:],Sigma[m,:,:])\n",
        "  iter = 200                           # iter: number of iterations/function evaluations \n",
        "  for t in range(iter):\n",
        "\n",
        "    y_t = syn_fun(x_t.T, syn_function, tau)\n",
        "    y_list.append(y_t)\n",
        "    obj_val = obj_syn_fun(x_t.T, syn_function)\n",
        "    obj_f_list.append(obj_val)\n",
        "\n",
        "    # Choose specific expert based on w pmf \n",
        "    w_pmf = np.squeeze(w_vec)\n",
        "    ind_exp = np.random.choice(np.arange(1, M+1), p=w_pmf)\n",
        "\n",
        "    # Sample from posterior \n",
        "    theta_samp = gauss_sample(theta_t[ind_exp-1,:,:],Sigma[ind_exp-1,:,:])\n",
        " \n",
        "    # Acquisition function theta_samp * RF  (Thomson sampling)\n",
        "    # Optimize acquisition function to obtain x_{t+1}\n",
        "\n",
        "    [x_t, obj_x_t] = optimize_x(theta_samp,V[ind_exp-1,:,:],D,X0_points,bnds)\n",
        "    x_t = np.array(x_t)\n",
        "    x_t = np.reshape(x_t,(len_x,1))\n",
        "    X_list = np.vstack((X_list, x_t.T))\n",
        "    # Estimate new kernels hyperparams using all data gathered so far\n",
        "    ker_vars,nu_vec,tau_vec = full_Bayesian_hyperparams(X_list,params,num_samples=11,warmup_steps=80,num_points= X_list.shape[0])\n",
        "    # Initialization of theta_t of posterior for all M experts\n",
        "    theta_t = np.zeros((M,2*D,1))\n",
        "    Sigma = np.zeros((M,2*D,2*D))\n",
        "    for m in range(M):\n",
        "      Sigma[m,:,:] = nu_vec[0,m]*np.identity(2*D)\n",
        "    V = np.zeros((M,D,len_x))\n",
        "    for m in range(M):\n",
        "      # Generate vectors v for the RF for all M experts\n",
        "      V[m,:,:] = RF_gen(len_x,D,ker_vars[0,m])       \n",
        "      # Update theta and Sigma of posterior for all M experts using all gathered data so far\n",
        "      theta_t[m,:,:], Sigma[m,:,:] = post_update_2(X_list,V[m,:,:],D,sigma_bias,tau_init[0],theta_t[m,:,:],Sigma[m,:,:])\n",
        "\n",
        "  return x_t,y_pred,obj_f_list,var_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def update_weights_EI(log_w):\n",
        "  log_w_aux = -np.sort(-log_w)\n",
        "  log_w_aux_temp = log_w_aux[0]+np.log(1+np.sum(np.exp(log_w_aux[1:]-log_w_aux[0])))\n",
        "  w = np.exp(log_w-log_w_aux_temp)\n",
        "  return w\n",
        "\n",
        "\n",
        "def BO_step_EI(train_X,train_Y, low_bound, up_bound, syn_function, tau):\n",
        "  \n",
        "  train_Y2 = torch.squeeze(train_Y)\n",
        "\n",
        "  # Botorch GP\n",
        "  gp = SingleTaskGP(train_X, train_Y)\n",
        "  mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "  fit_gpytorch_model(mll)\n",
        "\n",
        "  # acquistion functions\n",
        "\n",
        "  EI = ExpectedImprovement(gp, best_f= torch.max(train_Y))\n",
        "\n",
        "  #Optimize acquisition functions\n",
        "  \n",
        "  bounds = torch.stack([torch.zeros(train_X.shape[1])+torch.tensor(low_bound), torch.zeros(train_X.shape[1])+torch.tensor(up_bound)])\n",
        "  candidate_EI, acq_value_EI = optimize_acqf(\n",
        "    EI, bounds=bounds, q=1, num_restarts=5, raw_samples=20,\n",
        "  )\n",
        "  candidate_EI_num = candidate_EI.numpy()\n",
        "  candidate_EI_num = np.float64(candidate_EI_num)\n",
        "  score_EI = obj_syn_fun(candidate_EI_num, syn_function)\n",
        "  obs_EI = syn_fun(candidate_EI_num, syn_function, tau)\n",
        "  score_EI = np.array(score_EI)\n",
        "  obs_EI = np.array(obs_EI)\n",
        "  score_EI = torch.from_numpy(score_EI)\n",
        "  obs_EI = torch.from_numpy(obs_EI)\n",
        "\n",
        "\n",
        "  return candidate_EI, score_EI, obs_EI\n",
        "\n",
        "\n",
        "def BO_EI(train_X,train_Y, low_bound, up_bound, syn_function):\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]\n",
        "  temp = train_X.numpy() \n",
        "  F_eval = obj_syn_fun(temp, syn_function)\n",
        "  F_eval = np.array(F_eval)\n",
        "  F_eval = torch.from_numpy(F_eval)\n",
        "  F_eval = F_eval[0:2]\n",
        "  iter = 200\n",
        "  for j in range(iter):\n",
        "    [Xstar, sc, obs] = BO_step_EI(train_X,train_Y,low_bound,up_bound, syn_function, tau)\n",
        "    train_X = torch.cat((train_X,Xstar),0)\n",
        "    obs = torch.reshape(obs ,(1,1))\n",
        "    train_Y = torch.cat((train_Y,obs),0)\n",
        "    F_eval = torch.cat((F_eval, sc))\n",
        "  obj_f_list = F_eval.numpy()\n",
        "  return obj_f_list\n",
        "\n",
        "def BO_step_EI_RBF(train_X,train_Y, low_bound, up_bound, syn_function, tau):\n",
        "  \n",
        "  train_Y2 = torch.squeeze(train_Y)\n",
        "\n",
        "  # Botorch GP\n",
        "  cov_module = gpytorch.kernels.RBFKernel()\n",
        "  gp = SingleTaskGP(train_X, train_Y, covar_module = cov_module)\n",
        "  mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "  fit_gpytorch_model(mll)\n",
        "  \n",
        "\n",
        "  # acquistion functions\n",
        "\n",
        "  EI = ExpectedImprovement(gp, best_f= torch.max(train_Y))\n",
        "\n",
        "  #Optimize acquisition functions\n",
        "  \n",
        "  bounds = torch.stack([torch.zeros(train_X.shape[1])+torch.tensor(low_bound), torch.zeros(train_X.shape[1])+torch.tensor(up_bound)])\n",
        "  candidate_EI, acq_value_EI = optimize_acqf(\n",
        "    EI, bounds=bounds, q=1, num_restarts=5, raw_samples=20,\n",
        "  )\n",
        "  candidate_EI_num = candidate_EI.numpy()\n",
        "  candidate_EI_num = np.float64(candidate_EI_num)\n",
        "  score_EI = obj_syn_fun(candidate_EI_num, syn_function)\n",
        "  obs_EI = syn_fun(candidate_EI_num, syn_function, tau)\n",
        "  score_EI = np.array(score_EI)\n",
        "  obs_EI = np.array(obs_EI)\n",
        "  score_EI = torch.from_numpy(score_EI)\n",
        "  obs_EI = torch.from_numpy(obs_EI)\n",
        "\n",
        "\n",
        "  return candidate_EI, score_EI, obs_EI\n",
        "\n",
        "def BO_EI_RBF(train_X,train_Y, low_bound, up_bound, syn_function):\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]\n",
        "  temp = train_X.numpy() \n",
        "  F_eval = obj_syn_fun(temp, syn_function)\n",
        "  F_eval = np.array(F_eval)\n",
        "  F_eval = torch.from_numpy(F_eval)\n",
        "  F_eval = F_eval[0:2]\n",
        "  iter = 200\n",
        "  for j in range(iter):\n",
        "    [Xstar, sc, obs] = BO_step_EI_RBF(train_X,train_Y,low_bound,up_bound, syn_function, tau)\n",
        "    train_X = torch.cat((train_X,Xstar),0)\n",
        "    obs = torch.reshape(obs ,(1,1))\n",
        "    train_Y = torch.cat((train_Y,obs),0)\n",
        "    F_eval = torch.cat((F_eval, sc))\n",
        "  obj_f_list = F_eval.numpy()\n",
        "  return obj_f_list\n",
        "\n",
        "def BO_step_EI_Matern(train_X,train_Y, low_bound, up_bound, syn_function, tau, nu_val):\n",
        "  \n",
        "  train_Y2 = torch.squeeze(train_Y)\n",
        "\n",
        "  # Botorch GP\n",
        "  cov_module = gpytorch.kernels.MaternKernel(nu=nu_val)\n",
        "  gp = SingleTaskGP(train_X, train_Y, covar_module = cov_module)\n",
        "  mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "  fit_gpytorch_model(mll)\n",
        "  \n",
        "\n",
        "  # acquistion functions\n",
        "\n",
        "  EI = ExpectedImprovement(gp, best_f= torch.max(train_Y))\n",
        "\n",
        "  #Optimize acquisition functions\n",
        "  \n",
        "  bounds = torch.stack([torch.zeros(train_X.shape[1])+torch.tensor(low_bound), torch.zeros(train_X.shape[1])+torch.tensor(up_bound)])\n",
        "  candidate_EI, acq_value_EI = optimize_acqf(\n",
        "    EI, bounds=bounds, q=1, num_restarts=5, raw_samples=20,\n",
        "  )\n",
        "  candidate_EI_num = candidate_EI.numpy()\n",
        "  candidate_EI_num = np.float64(candidate_EI_num)\n",
        "  score_EI = obj_syn_fun(candidate_EI_num, syn_function)\n",
        "  obs_EI = syn_fun(candidate_EI_num, syn_function, tau)\n",
        "  score_EI = np.array(score_EI)\n",
        "  obs_EI = np.array(obs_EI)\n",
        "  score_EI = torch.from_numpy(score_EI)\n",
        "  obs_EI = torch.from_numpy(obs_EI)\n",
        "\n",
        "\n",
        "  return candidate_EI, score_EI, obs_EI\n",
        "\n",
        "def BO_EI_Matern(train_X,train_Y, low_bound, up_bound, syn_function, nu_val):\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]\n",
        "  temp = train_X.numpy() \n",
        "  F_eval = obj_syn_fun(temp, syn_function)\n",
        "  F_eval = np.array(F_eval)\n",
        "  F_eval = torch.from_numpy(F_eval)\n",
        "  F_eval = F_eval[0:2]\n",
        "  iter = 200\n",
        "  for j in range(iter):\n",
        "    [Xstar, sc, obs] = BO_step_EI_Matern(train_X,train_Y,low_bound,up_bound, syn_function, tau, nu_val)\n",
        "    train_X = torch.cat((train_X,Xstar),0)\n",
        "    obs = torch.reshape(obs ,(1,1))\n",
        "    train_Y = torch.cat((train_Y,obs),0)\n",
        "    F_eval = torch.cat((F_eval, sc))\n",
        "  obj_f_list = F_eval.numpy()\n",
        "  return obj_f_list\n",
        "\n",
        "def BO_step_EI_ensemble(train_X,train_Y, low_bound, up_bound, syn_function, tau):\n",
        "  train_Y2 = torch.squeeze(train_Y)\n",
        "  M = 4\n",
        "  #1st EI learner \n",
        "  gp1 = SingleTaskGP(train_X, train_Y)\n",
        "  mll1 = ExactMarginalLogLikelihood(gp1.likelihood, gp1)\n",
        "  fit_gpytorch_model(mll1)\n",
        "  mloglike1 = mll1(gp1(train_X), train_Y2).item()\n",
        "\n",
        "  #2nd EI learner \n",
        "  cov_module2 = gpytorch.kernels.RBFKernel()\n",
        "  gp2 = SingleTaskGP(train_X, train_Y, covar_module = cov_module2)\n",
        "  mll2 = ExactMarginalLogLikelihood(gp2.likelihood, gp2)\n",
        "  fit_gpytorch_model(mll2)\n",
        "  mloglike2 = mll2(gp2(train_X), train_Y2).item()\n",
        "\n",
        "  #3rd EI learner \n",
        "  cov_module3 = gpytorch.kernels.MaternKernel(nu=2.5)\n",
        "  gp3 = SingleTaskGP(train_X, train_Y, covar_module = cov_module3)\n",
        "  mll3 = ExactMarginalLogLikelihood(gp3.likelihood, gp3)\n",
        "  fit_gpytorch_model(mll3)\n",
        "  mloglike3 = mll3(gp3(train_X), train_Y2).item()\n",
        "\n",
        "  #4rd EI learner \n",
        "  cov_module4 = gpytorch.kernels.MaternKernel(nu=1.5)\n",
        "  gp4 = SingleTaskGP(train_X, train_Y, covar_module = cov_module4)\n",
        "  mll4 = ExactMarginalLogLikelihood(gp4.likelihood, gp4)\n",
        "  fit_gpytorch_model(mll4)\n",
        "  mloglike4 = mll4(gp4(train_X), train_Y2).item()\n",
        "\n",
        "  w_vec = np.array([mloglike1,mloglike2,mloglike3,mloglike4])\n",
        "  w_vec = update_weights_EI(w_vec)\n",
        "  w_pmf = np.squeeze(w_vec)\n",
        "  #print(w_pmf)\n",
        "  ind_exp = np.random.choice(np.arange(1, M+1), p=w_pmf)\n",
        "  \n",
        "  if ind_exp == 1:\n",
        "    gp = gp1\n",
        "  elif ind_exp == 2:\n",
        "    gp = gp2\n",
        "  elif ind_exp == 3:\n",
        "    gp = gp3\n",
        "  else:\n",
        "    gp = gp4  \n",
        "\n",
        "  EI = ExpectedImprovement(gp, best_f= torch.max(train_Y))\n",
        "\n",
        "  #Optimize acquisition functions\n",
        "  \n",
        "  bounds = torch.stack([torch.zeros(train_X.shape[1])+torch.tensor(low_bound), torch.zeros(train_X.shape[1])+torch.tensor(up_bound)])\n",
        "  candidate_EI, acq_value_EI = optimize_acqf(\n",
        "    EI, bounds=bounds, q=1, num_restarts=5, raw_samples=20,\n",
        "  )\n",
        "  candidate_EI_num = candidate_EI.numpy()\n",
        "  candidate_EI_num = np.float64(candidate_EI_num)\n",
        "  score_EI = obj_syn_fun(candidate_EI_num, syn_function)\n",
        "  obs_EI = syn_fun(candidate_EI_num, syn_function, tau)\n",
        "  score_EI = np.array(score_EI)\n",
        "  obs_EI = np.array(obs_EI)\n",
        "  score_EI = torch.from_numpy(score_EI)\n",
        "  obs_EI = torch.from_numpy(obs_EI)\n",
        "\n",
        "\n",
        "  return candidate_EI, score_EI, obs_EI\n",
        "\n",
        "\n",
        "def BO_EI_ensemble(train_X,train_Y, low_bound, up_bound, syn_function):\n",
        "  tau_vec = params[\"tau\"]\n",
        "  tau = tau_vec[0]\n",
        "  M = 4\n",
        "  temp = train_X.numpy() \n",
        "  F_eval = obj_syn_fun(temp, syn_function)\n",
        "  F_eval = np.array(F_eval)\n",
        "  F_eval = torch.from_numpy(F_eval)\n",
        "  F_eval = F_eval[0:2]\n",
        "  iter = 200  \n",
        "  for j in range(iter):\n",
        "    [Xstar, sc, obs] = BO_step_EI_ensemble(train_X,train_Y,low_bound,up_bound, syn_function, tau)\n",
        "    train_X = torch.cat((train_X,Xstar),0)\n",
        "    obs = torch.reshape(obs ,(1,1))\n",
        "    train_Y = torch.cat((train_Y,obs),0)\n",
        "    F_eval = torch.cat((F_eval, sc))\n",
        "  obj_f_list = F_eval.numpy()\n",
        "  return obj_f_list  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GF5vDI7vkFvu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run experiments for a single run"
      ],
      "metadata": {
        "id": "4NxAbGCxEVAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ4D6kWHNXOM"
      },
      "outputs": [],
      "source": [
        "aver_regret_gpfull = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t,y_pred,obj_f_list_gpfull,var_pred = main_full_Bayesian_GP(X,y,params,X0_points,bnds,syn_function)\n",
        "  obj_f_list_gpfull = np.squeeze(obj_f_list_gpfull)\n",
        "  regret_list_gpfull = simple_regret(obj_f_list_gpfull,benchmark_value)\n",
        "  aver_regret_gpfull = aver_regret_gpfull + regret_list_gpfull\n",
        "aver_regret_gpfull = aver_regret_gpfull/n_runs\n",
        "print(\"Final value of full Bayesian GP:\", aver_regret_gpfull[199])\n",
        "print(aver_regret_gpfull)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe9Zjsby-iWj"
      },
      "outputs": [],
      "source": [
        "aver_regret_gpARD = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t,y_pred,obj_f_list_gpARD,var_pred = main_GP_ARD(X,y,params,X0_points,bnds,syn_function, reinit_key)\n",
        "  obj_f_list_gpARD = np.squeeze(obj_f_list_gpARD)\n",
        "  regret_list_gpARD = simple_regret(obj_f_list_gpARD,benchmark_value)\n",
        "  aver_regret_gpARD = aver_regret_gpARD + regret_list_gpARD\n",
        "aver_regret_gpARD = aver_regret_gpARD/n_runs\n",
        "print(\"Final value of GP with ARD:\", aver_regret_gpARD[199])\n",
        "print(aver_regret_gpARD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvXZn8MNZn1n"
      },
      "outputs": [],
      "source": [
        "aver_regret_gpmult = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t,y_pred,obj_f_list_gpmult,var_pred = main_GP_matern(X,y,params,X0_points,bnds,syn_function, reinit_key)\n",
        "  obj_f_list_gpmult = np.squeeze(obj_f_list_gpmult)\n",
        "  regret_list_gpmult = simple_regret(obj_f_list_gpmult,benchmark_value)\n",
        "  aver_regret_gpmult = aver_regret_gpmult + regret_list_gpmult\n",
        "aver_regret_gpmult = aver_regret_gpmult/n_runs\n",
        "print(\"Final value of GP with Mattern:\", aver_regret_gpmult[199])\n",
        "print(aver_regret_gpmult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE5kSSernHP3"
      },
      "outputs": [],
      "source": [
        "aver_regret_gpmult = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t,y_pred,obj_f_list_gpmult,var_pred = main_GP_matern2(X,y,params,X0_points,bnds,syn_function, reinit_key)\n",
        "  obj_f_list_gpmult = np.squeeze(obj_f_list_gpmult)\n",
        "  regret_list_gpmult = simple_regret(obj_f_list_gpmult,benchmark_value)\n",
        "  aver_regret_gpmult = aver_regret_gpmult + regret_list_gpmult\n",
        "aver_regret_gpmult = aver_regret_gpmult/n_runs\n",
        "print(\"Final value of GP with Mattern 2:\", aver_regret_gpmult[199])\n",
        "print(aver_regret_gpmult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYt0Wu9YHOm8"
      },
      "outputs": [],
      "source": [
        "aver_regret_exp3 = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t,y_pred,obj_f_list_exp3,var_pred = main_EXP3BO(X,y,params,X0_points,bnds,syn_function,reinit_key,benchmark_value)\n",
        "  obj_f_list_exp3 = np.squeeze(obj_f_list_exp3)\n",
        "  regret_list_exp3 = simple_regret(obj_f_list_exp3,benchmark_value)\n",
        "  aver_regret_exp3 = aver_regret_exp3 + regret_list_exp3\n",
        "aver_regret_exp3 = aver_regret_exp3/n_runs\n",
        "print(\"Final value of EXP3BO:\", aver_regret_exp3[199])\n",
        "print(aver_regret_exp3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyBz_w7wjYGa"
      },
      "outputs": [],
      "source": [
        "aver_regret_exp3 = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t,y_pred,obj_f_list_exp3,var_pred = main_EXP3BO_ardrbfmatern(X,y,params,X0_points,bnds,syn_function,reinit_key,benchmark_value)\n",
        "  obj_f_list_exp3 = np.squeeze(obj_f_list_exp3)\n",
        "  regret_list_exp3 = simple_regret(obj_f_list_exp3,benchmark_value)\n",
        "  aver_regret_exp3 = aver_regret_exp3 + regret_list_exp3\n",
        "aver_regret_exp3 = aver_regret_exp3/n_runs\n",
        "print(\"Final value of EXP3BO with ARD:\", aver_regret_exp3[199])\n",
        "print(aver_regret_exp3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kULJ5HEtzjIL"
      },
      "outputs": [],
      "source": [
        "x_t,y_pred,obj_f_list_mult2,var_pred = main_EGP_rbfmaternard(X,y,params,X0_points,bnds,syn_function, reinit_key)\n",
        "obj_f_list_mult2 = np.squeeze(obj_f_list_mult2)\n",
        "n = np.arange(1,len(obj_f_list_mult2)+1)\n",
        "plt.plot(n,obj_f_list_mult2,n,benchmark_value*np.ones(len(obj_f_list_mult2)))\n",
        "plt.title(syn_function + str(' - EGP with diff. kern. types'))\n",
        "plt.ylabel('objective function value')\n",
        "plt.xlabel('iteration')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipnfW3FfznUL"
      },
      "outputs": [],
      "source": [
        "aver_regret_mult2 = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t,y_pred,obj_f_list_mult2,var_pred = main_EGP_rbfmaternard(X,y,params,X0_points,bnds,syn_function,reinit_key)\n",
        "  obj_f_list_mult2 = np.squeeze(obj_f_list_mult2)\n",
        "  regret_list_mult2 = simple_regret(obj_f_list_mult2,benchmark_value)\n",
        "  aver_regret_mult2 = aver_regret_mult2 + regret_list_mult2\n",
        "aver_regret_mult2 = aver_regret_mult2/n_runs\n",
        "print(\"Final value of EGP with different kernel types including ARD:\", aver_regret_mult2[199])\n",
        "print(aver_regret_mult2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USS3o6qNccnW"
      },
      "outputs": [],
      "source": [
        "aver_regret = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t,y_pred,obj_f_list,var_pred = main_GP(X,y,params,X0_points,bnds,syn_function,reinit_key)\n",
        "  obj_f_list = np.squeeze(obj_f_list)\n",
        "  regret_list = simple_regret(obj_f_list,benchmark_value)\n",
        "  aver_regret = aver_regret + regret_list\n",
        "aver_regret = aver_regret/n_runs\n",
        "print(\"Final value of GP:\", aver_regret[199])\n",
        "print(aver_regret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckfG4eNGdAFE"
      },
      "outputs": [],
      "source": [
        "aver_regret2 = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t2,y_pred2,obj_f_list_egp,var_pred2 = main_EGP(X,y,params,X0_points,bnds,syn_function,reinit_key)\n",
        "  obj_f_list_egp = np.squeeze(obj_f_list_egp)\n",
        "  regret_list2 = simple_regret(obj_f_list_egp,benchmark_value)\n",
        "  aver_regret2 = aver_regret2 + regret_list2\n",
        "aver_regret2 = aver_regret2/n_runs\n",
        "print(\"Final value of EGP:\", aver_regret2[199])\n",
        "print(aver_regret2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJh1SVYSdh-o"
      },
      "outputs": [],
      "source": [
        "aver_regret3 = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t3,y_pred3,obj_f_list_bandit,var_pred3 = main_BanditBO(X,y,params,X0_points,bnds,syn_function,reinit_key)\n",
        "  obj_f_list_bandit = np.squeeze(obj_f_list_bandit)\n",
        "  regret_list3 = simple_regret(obj_f_list_bandit,benchmark_value)\n",
        "  aver_regret3 = aver_regret3 + regret_list3\n",
        "aver_regret3 = aver_regret3/n_runs\n",
        "print(\"Final value of BanditBO:\", aver_regret3[199])\n",
        "print(aver_regret3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwNGjO_zjnsE"
      },
      "outputs": [],
      "source": [
        "aver_regret4 = np.zeros(200)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  x_t3,y_pred3,obj_f_list_bandit,var_pred3 = main_BanditBO_ardrbfmatern(X,y,params,X0_points,bnds,syn_function,reinit_key)\n",
        "  obj_f_list_bandit = np.squeeze(obj_f_list_bandit)\n",
        "  regret_list4 = simple_regret(obj_f_list_bandit,benchmark_value)\n",
        "  aver_regret4 = aver_regret4 + regret_list4\n",
        "aver_regret4 = aver_regret4/n_runs\n",
        "print(\"Final value of BanditBO with ARD:\", aver_regret4[199])\n",
        "print(aver_regret4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = torch.from_numpy(X)\n",
        "train_Y = torch.from_numpy(y)\n",
        "train_Y = torch.reshape(train_Y,(10,1))"
      ],
      "metadata": {
        "id": "5l-dYlptqt4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aver_regret_EI = np.zeros(202)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  obj_f_list_EI = BO_EI_ensemble(train_X,train_Y, low_bound, up_bound, syn_function)\n",
        "  obj_f_list_EI = np.squeeze(obj_f_list_EI)\n",
        "  regret_list_EI = simple_regret(obj_f_list_EI,benchmark_value)\n",
        "  aver_regret_EI = aver_regret_EI + regret_list_EI\n",
        "aver_regret_EI = aver_regret_EI/n_runs\n",
        "print(\"Final value of GP with EI-ensemble:\", aver_regret_EI[199])\n",
        "print(aver_regret_EI)"
      ],
      "metadata": {
        "id": "ROb0hWtPktZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aver_regret_EI = np.zeros(202)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  obj_f_list_EI = BO_EI(train_X,train_Y, low_bound, up_bound, syn_function)\n",
        "  obj_f_list_EI = np.squeeze(obj_f_list_EI)\n",
        "  regret_list_EI = simple_regret(obj_f_list_EI,benchmark_value)\n",
        "  aver_regret_EI = aver_regret_EI + regret_list_EI\n",
        "aver_regret_EI = aver_regret_EI/n_runs\n",
        "print(\"Final value of GP with EI-ARD (default):\", aver_regret_EI[199])\n",
        "print(aver_regret_EI)"
      ],
      "metadata": {
        "id": "1bYcY0DQkwiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aver_regret_EI = np.zeros(202)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  obj_f_list_EI = BO_EI_RBF(train_X,train_Y, low_bound, up_bound, syn_function)\n",
        "  obj_f_list_EI = np.squeeze(obj_f_list_EI)\n",
        "  regret_list_EI = simple_regret(obj_f_list_EI,benchmark_value)\n",
        "  aver_regret_EI = aver_regret_EI + regret_list_EI\n",
        "aver_regret_EI = aver_regret_EI/n_runs\n",
        "print(\"Final value of GP with EI-RBF:\", aver_regret_EI[199])\n",
        "print(aver_regret_EI)"
      ],
      "metadata": {
        "id": "UPv2153RkzEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aver_regret_EI = np.zeros(202)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  obj_f_list_EI = BO_EI_Matern(train_X,train_Y, low_bound, up_bound, syn_function, 2.5)\n",
        "  obj_f_list_EI = np.squeeze(obj_f_list_EI)\n",
        "  regret_list_EI = simple_regret(obj_f_list_EI,benchmark_value)\n",
        "  aver_regret_EI = aver_regret_EI + regret_list_EI\n",
        "aver_regret_EI = aver_regret_EI/n_runs\n",
        "print(\"Final value of GP with EI-Matern 1:\", aver_regret_EI[199])\n",
        "print(aver_regret_EI)"
      ],
      "metadata": {
        "id": "zU-NmM0dk6XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aver_regret_EI = np.zeros(202)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  obj_f_list_EI = BO_EI_Matern(train_X,train_Y, low_bound, up_bound, syn_function, 1.5)\n",
        "  obj_f_list_EI = np.squeeze(obj_f_list_EI)\n",
        "  regret_list_EI = simple_regret(obj_f_list_EI,benchmark_value)\n",
        "  aver_regret_EI = aver_regret_EI + regret_list_EI\n",
        "aver_regret_EI = aver_regret_EI/n_runs\n",
        "print(\"Final value of GP with EI-Matern 2:\", aver_regret_EI[199])\n",
        "print(aver_regret_EI)"
      ],
      "metadata": {
        "id": "Ye4unaa9lByp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}